{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, cut, DataFrame\n",
    "from numpy import array, unique, log2, inf, append, where, square, random, concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from pprint import pprint\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'catalog4/cat4.csv'\n",
    "df = read_csv(file_path)\n",
    "df.head()\n",
    "y = array(df['class'])\n",
    "# df = df.rename(columns={\"class\": \"label\"})\n",
    "if 'cat2.csv' in file_path:\n",
    "    df.drop(\"Unnamed: 0.1\", axis=1, inplace=True)\n",
    "df.drop([\"Unnamed: 0\", \"galex_objid\", \"sdss_objid\", \"pred\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketize(dataframe, col_headers, bucket_size):\n",
    "    assert len(col_headers) == len(bucket_size)\n",
    "    no_of_columns = len(col_headers)\n",
    "    for col in range(no_of_columns):\n",
    "        labels = array([(x + 1) for x in range(bucket_size[col])])\n",
    "        temp = cut(dataframe[col_headers[col]], bucket_size[col], labels=labels)\n",
    "        dataframe.drop(col_headers[col], inplace=True, axis=1)\n",
    "        dataframe[col_headers[col]] = temp\n",
    "    return dataframe\n",
    "\n",
    "y = df['class']\n",
    "redshift = array(df['spectrometric_redshift'])\n",
    "X = df.drop(['class', 'spectrometric_redshift'], axis=1)\n",
    "df_new = bucketize(X, X.columns, [10 for x in range(len(X.columns))])\n",
    "df_new['class'] = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, redshift_train, redshift_test = train_test_split(df_new, redshift, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": 7,
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(df, n_samples, replace=True, random_state=None):\n",
    "    temp = DataFrame()\n",
    "    length_df = len(df)\n",
    "#     base_size = 10\n",
    "#     if replace == False:\n",
    "#         assert n_samples <= length_df\n",
    "#         return df.sample(n=n_samples, random_state=random_state)\n",
    "#     else:\n",
    "#         while n_samples > base_size:\n",
    "#             temp = temp.append(df.sample(n=base_size), ignore_index=True)\n",
    "#             n_samples -= 10\n",
    "#         temp = temp.append(df.sample(n=n_samples))\n",
    "#     indices = df.index\n",
    "    for index in range(n_samples):\n",
    "        choice = random.choice(length_df)\n",
    "        temp = temp.append(df.iloc[choice])\n",
    "    return temp\n",
    "\n",
    "def over_sampling(df):\n",
    "    df_0 = df[df['class'] == 0]\n",
    "    df_1 = df[df['class'] == 1]\n",
    "    \n",
    "    length_1 = len(df_1)\n",
    "    length_0 = len(df_0)\n",
    "    for i in range(length_1):\n",
    "        index = randrange(length_0)\n",
    "        df_1 = df_1.append(df_0.iloc[index])\n",
    "    return df_1\n",
    "\n",
    "# df_0 = train[train['class'] == 0]\n",
    "# df_1 = train[train['class'] == 1]\n",
    "# train_resampled = over_sampling(train)\n",
    "train = over_sampling(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def under_sampling(df):\n",
    "    min_class_len = len(df[df['class'] == 0])\n",
    "    major_class_ind = df[df['class'] == 1].index\n",
    "    rand_maj_ind = random.choice(major_class_ind, min_class_len, replace = False)\n",
    "    min_class_ind = df[df['class'] == 0].index\n",
    "    \n",
    "    dropped_data = []\n",
    "    for i in df.index:\n",
    "        if (i not in rand_maj_ind) and (i not in min_class_ind):\n",
    "            dropped_data.append(i)\n",
    "#     print(dropped_data)\n",
    "    \n",
    "    under_sample_ind = concatenate([min_class_ind,rand_maj_ind])\n",
    "    under_sample = df.loc[under_sample_ind]\n",
    "    \n",
    "    return under_sample\n",
    "\n",
    "under_sample = under_sampling(df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 8,
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting class and bucketizing the train data\n",
    "y_train = array(train['class'])\n",
    "train.drop('class', inplace=True, axis=1)\n",
    "X_train = array(train)\n",
    "\n",
    "# extracting class and bucketizing the test data\n",
    "y_test = array(test['class'])\n",
    "test.drop('class', inplace=True, axis=1)\n",
    "X_test = array(test)\n",
    "\n",
    "# y = array(under_sample['class'])\n",
    "# under_sample.drop('class', inplace=True, axis=1)\n",
    "# temp = bucketize(under_sample, under_sample.columns, [7 for x in range(len(under_sample.columns))])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 9,
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['u', 'g', 'r', 'i', 'z', 'extinction_u', 'extinction_g', 'extinction_r',\n",
       "       'extinction_i', 'extinction_z', 'nuv_mag', 'fuv_mag', 'nuv-u', 'nuv-g',\n",
       "       'nuv-r', 'nuv-i', 'nuv-z', 'u-g', 'u-r', 'u-i', 'u-z', 'g-r', 'g-i',\n",
       "       'g-z', 'r-i', 'r-z', 'i-z', 'fuv-nuv', 'fuv-u', 'fuv-g', 'fuv-r',\n",
       "       'fuv-i', 'fuv-z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 46,
=======
   "execution_count": 10,
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \n",
    "    def __init__(self, max_depth=5, min_samples=2):\n",
    "        self.counter = 0\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        \n",
    "    def check_purity(self, y):\n",
    "        if len(unique(y)) == 1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def classify_data(self, y):\n",
    "        unique_classes, counts_unique_classes = unique(y, return_counts=True)\n",
    "\n",
    "        index = where(counts_unique_classes == max(counts_unique_classes))[0][0]\n",
    "        classification = unique_classes[index]\n",
    "\n",
    "        return classification\n",
    "    \n",
    "    def get_potential_splits(self, X):\n",
    "    \n",
    "        potential_splits = {}\n",
    "        n_columns = len(X[0])\n",
    "        for column_index in range(n_columns):  \n",
    "            potential_splits[column_index] = set()\n",
    "            values = X[:, column_index]\n",
    "            unique_values = unique(values)\n",
    "\n",
    "            for index in range(1, len(unique_values)):\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "\n",
    "                potential_splits[column_index].add(potential_split)\n",
    "\n",
    "        return potential_splits\n",
    "    \n",
    "    def split_data(self, X, y, split_column, split_value):\n",
    "    \n",
    "        no_of_columns = len(X[0]) + 1\n",
    "        split_column_values = X[:, split_column]\n",
    "        data = append(X, y.reshape(len(X), 1), axis=1)\n",
    "        data_below = data[data[:, split_column] < split_value]\n",
    "        data_above = data[data[:, split_column] >= split_value]\n",
    "\n",
    "        return data_below, data_above\n",
    "    \n",
    "    def calculate_entropy(self, label_column):\n",
    "\n",
    "        _, counts = unique(label_column, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        return 1 - sum(square(probabilities))\n",
    "    \n",
    "    def calculate_overall_entropy(self, data_below, data_above):\n",
    "    \n",
    "        n = len(data_below) + len(data_above)\n",
    "        p_data_below = len(data_below) / n\n",
    "        p_data_above = len(data_above) / n\n",
    "\n",
    "        overall_entropy =  (p_data_below * self.calculate_entropy(data_below[:, -1]) \n",
    "                          + p_data_above * self.calculate_entropy(data_above[:, -1]))\n",
    "\n",
    "        return overall_entropy\n",
    "    \n",
    "    def determine_best_split(self, X, y, potential_splits):    \n",
    "        overall_entropy = inf\n",
    "        for column_index in potential_splits:\n",
    "            for value in potential_splits[column_index]:\n",
    "                data_below, data_above = self.split_data(X, y, split_column=column_index, split_value=value)\n",
    "                current_overall_entropy = self.calculate_overall_entropy(data_below, data_above)\n",
    "\n",
    "                if current_overall_entropy <= overall_entropy:\n",
    "                    overall_entropy = current_overall_entropy\n",
    "                    best_split_column = column_index\n",
    "                    best_split_value = value\n",
    "\n",
    "        return best_split_column, best_split_value\n",
    "    \n",
    "    def build_tree(self, X, y):\n",
    "        if (self.check_purity(y)) or (len(X) < self.min_samples) or (self.counter == self.max_depth):\n",
    "            classification = self.classify_data(y)\n",
    "\n",
    "            return classification\n",
    "\n",
    "\n",
    "        # recursive part\n",
    "        else:    \n",
    "            self.counter += 1\n",
    "\n",
    "            # helper functions \n",
    "            potential_splits = self.get_potential_splits(X)\n",
    "            split_column, split_value = self.determine_best_split(X, y, potential_splits)\n",
    "            data_below, data_above = self.split_data(X, y, split_column, split_value)\n",
    "\n",
    "            question = \"column_{} <= {}\".format(split_column, split_value)\n",
    "            sub_tree = {question: []}\n",
    "\n",
    "            # find answers (recursion)\n",
    "            yes_answer = self.build_tree(data_below[:, :-1], data_below[:, -1])\n",
    "            no_answer = self.build_tree(data_above[:, :-1], data_above[:, -1])\n",
    "\n",
    "            # If the answers are the same, then there is no point in asking the qestion.\n",
    "            # This could happen when the data is classified even though it is not pure\n",
    "            # yet (min_samples or max_depth base case).\n",
    "            if yes_answer == no_answer:\n",
    "                sub_tree = yes_answer\n",
    "            else:\n",
    "                sub_tree[question].append(yes_answer)\n",
    "                sub_tree[question].append(no_answer)\n",
    "\n",
    "            return sub_tree\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        tree = self.build_tree(X, y)\n",
    "        end_time = time.time()\n",
    "        print(\"Time taken to construct the decision tree =\", end_time - start_time)\n",
    "        return tree\n",
    "    \n",
    "    def classify_example(self, example, tree):\n",
    "        question = list(tree.keys())[0]\n",
    "        feature_name, comparison_operator, value = question.split(\" \")\n",
    "        x = int(feature_name.split(\"_\")[1])\n",
    "\n",
    "        # ask question\n",
    "        if example[x] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "\n",
    "        # when at leaf node\n",
    "        if type(answer) != dict:\n",
    "            return answer\n",
    "\n",
    "        # when at an internal node\n",
    "        else:\n",
    "            residual_tree = answer\n",
    "            return self.classify_example(example, residual_tree)\n",
    "    \n",
    "    def predict(self, X_test, tree):\n",
    "        predictions = array([])\n",
    "        for example in X_test:\n",
    "            predictions = append(predictions, self.classify_example(example, tree))\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def comp_with_redshift(self, redshift, y_test, y_pred):\n",
    "        correct_count = 0\n",
    "        length = len(y_test)\n",
    "        z_1_test = z_2_test = z_3_test = array([])\n",
    "        z_1_pred = z_2_pred = z_3_pred = array([])\n",
    "        for index in range(length):\n",
    "            if redshift[index] <= 0.0033:\n",
    "                z_1_test = append(z_1_test, y_test[index])\n",
    "                z_1_pred = append(z_1_pred, y_pred[index])\n",
    "            elif redshift[index] >= 0.0033 and redshift[index] <= 0.004:\n",
    "                z_2_test = append(z_2_test, y_test[index])\n",
    "                z_2_pred = append(z_2_pred, y_pred[index])\n",
    "            else:\n",
    "                z_3_test = append(z_3_test, y_test[index])\n",
    "                z_3_pred = append(z_3_pred, y_pred[index])\n",
    "\n",
    "        print(\"Confusion matrix of range 1:\", confusion_matrix(z_1_test, z_1_pred), end='\\n')\n",
    "        print(\"Confusion matrix of range 2:\", confusion_matrix(z_2_test, z_2_pred), end='\\n')\n",
    "        print(\"Confusion matrix of range 3:\", confusion_matrix(z_3_test, z_3_pred), end='\\n')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 47,
=======
   "execution_count": 11,
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Time taken to construct the decision tree = 2.8671302795410156\n"
=======
      "Time taken to construct the decision tree = 5.243674039840698\n"
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(max_depth=5)\n",
    "\n",
    "decision_tree = tree.fit(X_train, y_train)\n",
    "predictions = tree.predict(X_test, decision_tree)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": 12,
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.67      0.66      3029\n",
      "           1       0.85      0.84      0.85      7010\n",
      "\n",
<<<<<<< HEAD
      "    accuracy                           0.79     10039\n",
      "   macro avg       0.75      0.76      0.75     10039\n",
      "weighted avg       0.79      0.79      0.79     10039\n",
=======
      "          0       0.69      0.79      0.74      3024\n",
      "          1       0.76      0.64      0.70      3021\n",
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
      "\n",
      "Confusion matrix of range 1: [[1998  986]\n",
      " [  12    8]]\n",
      "Confusion matrix of range 2: [[4 5]\n",
      " [0 0]]\n",
      "Confusion matrix of range 3: [[  24   12]\n",
      " [1087 5903]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))\n",
    "tree.comp_with_redshift(redshift_test, y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33463\n"
     ]
    }
   ],
   "source": [
    "print(len(redshift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.3\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> 54f882adb9865df365e0ac562f06ecbda62fbb58
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
